learning_rate:
  0.00004
learning_rate_warmup:
  200
optimizer:
  "adam"
nepochs:
  11
batch_size:
  256
resume:
  "" # .npz for mlx, .pth for torch
save_every:
  -1
num_layers:
  4
num_heads:
  4
emebedding_dim:
  1024
vocab_size:
  128
bin_size:
  128
seed:
  99
