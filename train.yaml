optimizer: "AdamW"
nepochs: 11
batch_size: 64
num_layers: 8
num_heads: 8
embedding_dim: 1024
vocab_size: 128
num_classes: 128
seed: 99

log_interval: 16
warmup_epochs: 0
warmup_learning_rate: 1.0e-6
learning_rate: 1.0e-3
final_learning_rate: 1.2e-5
weight_decay: 0.05
gradient_clipping_norm: 1.0
beta_1: 0.9
beta_2: 0.95
save_dir: "checkpoints"
train_dset_path: "datasetGen/train.db"
val_dset_path: "datasetGen/val.db"
test_dset_path: "datasetGen/test.db"
