iterations: 50
d_model: 512
d_input: 512
memory_length: 15
heads: 8
n_synch_out: 128
n_synch_action: 128
num_classes: 128
memory_hidden_dims: 128
vocab_size: 128
token_embed_dim: 512

optimizer: "AdamW"
nepochs: 3
batch_size: 64
target_batch_size: 1024
seed: 99
log_interval: 16
warmup_batches: 2500
warmup_learning_rate: 1.0e-6
learning_rate: 1.0e-3
final_learning_rate: 1.2e-5
weight_decay: 0.05
gradient_clipping_norm: 1.0
beta_1: 0.9
beta_2: 0.95
save_dir: "checkpoints"
train_dset_path: "dataset/test.db"
val_dset_path: "dataset/val.db"
test_dset_path: "dataset/test.db"
resume_checkpoint_path: "none"
