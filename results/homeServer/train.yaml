learning_rate:
  0.00004
optimizer:
  "adam"
nepochs:
  3
batch_size:
  128
resume:
  "torch_adam_4e-05_128_4_4_1024_128_128_epoch_1_batch_300000.pt" # .npz for mlx, .pth for torch
save_every:
  100000
num_layers:
  4
num_heads:
  4
emebedding_dim:
  1024
vocab_size:
  128
bin_size:
  128
seed:
  999
